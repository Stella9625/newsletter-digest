"""æŠ“å–å±‚ï¼šRSS è§£æ + å…¨æ–‡æŠ“å–"""

from __future__ import annotations

import logging
import time
from dataclasses import dataclass, field
from datetime import datetime, timezone, timedelta

import feedparser
import httpx
from bs4 import BeautifulSoup
from readability import Document

from src.config import (
    RSSSource,
    FETCH_WINDOW_HOURS,
    MIN_CONTENT_LENGTH,
    HTTP_TIMEOUT,
    USER_AGENT,
)

logger = logging.getLogger(__name__)


@dataclass
class Article:
    """ç»Ÿä¸€çš„æ–‡ç« æ•°æ®ç»“æ„"""
    url: str
    title: str
    author: str
    source_name: str
    published_at: datetime | None
    content: str  # åŸæ–‡å†…å®¹ï¼ˆHTML æˆ–çº¯æ–‡æœ¬ï¼‰
    summary_zh: str = ""
    tags: list[str] = field(default_factory=list)
    translation_zh: str = ""
    quotes: list[dict] = field(default_factory=list)  # é‡‘å¥åˆ—è¡¨ï¼Œæ¯ä¸ª dict å« en/zh
    tone: str = ""  # è¯­æ°”æ ‡æ³¨ï¼Œå¦‚ "ğŸ§ª å®éªŒè®°å½•"
    title_zh: str = ""  # ä¸­æ–‡æ ‡é¢˜


def _parse_published_time(entry) -> datetime | None:
    """ä» feedparser entry ä¸­æå–å‘å¸ƒæ—¶é—´"""
    # feedparser ä¼šå°†æ—¶é—´è§£æä¸º time.struct_time
    for attr in ("published_parsed", "updated_parsed"):
        parsed = getattr(entry, attr, None)
        if parsed:
            try:
                return datetime(*parsed[:6], tzinfo=timezone.utc)
            except (TypeError, ValueError):
                continue
    return None


def _clean_html(html: str) -> str:
    """æ¸…ç† HTMLï¼Œæå–çº¯æ–‡æœ¬ï¼Œä¿ç•™æ®µè½ç»“æ„"""
    soup = BeautifulSoup(html, "lxml")

    # ç§»é™¤ script/style æ ‡ç­¾
    for tag in soup(["script", "style"]):
        tag.decompose()

    # æå–æ–‡æœ¬å’Œå›¾ç‰‡ï¼Œç”¨æ¢è¡Œåˆ†éš”æ®µè½
    lines = []
    for element in soup.find_all(["p", "h1", "h2", "h3", "h4", "li", "blockquote", "pre", "img"]):
        # å›¾ç‰‡ï¼šä¿ç•™ä¸º markdown æ ¼å¼
        if element.name == "img":
            src = element.get("src", "")
            alt = element.get("alt", "")
            if src:
                lines.append(f"![{alt}]({src})")
            continue

        # æ®µè½å†…åµŒå›¾ç‰‡ä¹Ÿæå–å‡ºæ¥
        for img in element.find_all("img"):
            src = img.get("src", "")
            alt = img.get("alt", "")
            if src:
                lines.append(f"![{alt}]({src})")

        text = element.get_text(strip=True)
        if text:
            # ä¿ç•™æ ‡é¢˜å±‚çº§
            if element.name in ("h1", "h2", "h3", "h4"):
                lines.append(f"\n{'#' * int(element.name[1])} {text}\n")
            elif element.name == "li":
                lines.append(f"- {text}")
            elif element.name == "blockquote":
                lines.append(f"> {text}")
            elif element.name == "pre":
                lines.append(f"```\n{text}\n```")
            else:
                lines.append(text)

    result = "\n\n".join(lines)

    # å¦‚æœæå–ç»“æœå¤ªçŸ­ï¼Œå›é€€åˆ°ç®€å•æ–‡æœ¬æå–
    if len(result) < 100:
        result = soup.get_text(separator="\n", strip=True)

    return result


def _get_entry_content(entry) -> str:
    """ä» feedparser entry ä¸­æå–å†…å®¹ï¼ˆHTMLï¼‰"""
    # ä¼˜å…ˆå– content å­—æ®µï¼ˆé€šå¸¸æ˜¯å®Œæ•´å†…å®¹ï¼‰
    if hasattr(entry, "content") and entry.content:
        return entry.content[0].get("value", "")
    # å…¶æ¬¡å– summary
    if hasattr(entry, "summary") and entry.summary:
        return entry.summary
    # æœ€åå– description
    if hasattr(entry, "description") and entry.description:
        return entry.description
    return ""


def fetch_full_text(url: str) -> str:
    """æŠ“å–ç½‘é¡µå…¨æ–‡ï¼Œç”¨ readability æå–æ­£æ–‡"""
    try:
        with httpx.Client(timeout=HTTP_TIMEOUT, follow_redirects=True) as client:
            resp = client.get(url, headers={"User-Agent": USER_AGENT})
            resp.raise_for_status()

        doc = Document(resp.text)
        html = doc.summary()
        return _clean_html(html)

    except Exception as e:
        logger.warning(f"å…¨æ–‡æŠ“å–å¤±è´¥ {url}: {e}")
        return ""


def fetch_feeds(
    sources: list[RSSSource],
    window_hours: int = FETCH_WINDOW_HOURS,
) -> list[Article]:
    """
    éå† RSS æºï¼Œè§£æå¹¶è¿”å›æ—¶é—´çª—å£å†…çš„æ–°æ–‡ç« ã€‚
    å¦‚æœ RSS ä¸­çš„å†…å®¹å¤ªçŸ­ï¼Œè‡ªåŠ¨æŠ“å–å…¨æ–‡ã€‚
    """
    cutoff = datetime.now(timezone.utc) - timedelta(hours=window_hours)
    articles = []

    for source in sources:
        logger.info(f"æ­£åœ¨æŠ“å–: {source.name} ({source.url})")

        try:
            feed = feedparser.parse(source.url)
        except Exception as e:
            logger.error(f"RSS è§£æå¤±è´¥ [{source.name}]: {e}")
            continue

        if feed.bozo and not feed.entries:
            logger.warning(f"RSS è§£æå¼‚å¸¸ [{source.name}]: {feed.bozo_exception}")
            continue

        for entry in feed.entries:
            # æå–å‘å¸ƒæ—¶é—´
            pub_time = _parse_published_time(entry)

            # è¿‡æ»¤æ—¶é—´çª—å£å¤–çš„æ–‡ç« 
            if pub_time and pub_time < cutoff:
                continue

            # æå– URL
            url = getattr(entry, "link", "") or ""
            if not url:
                continue

            # æå–å†…å®¹
            raw_content = _get_entry_content(entry)
            content = _clean_html(raw_content) if raw_content else ""

            # å†…å®¹å¤ªçŸ­æ—¶æŠ“å–å…¨æ–‡
            if len(content) < MIN_CONTENT_LENGTH:
                logger.info(f"  å†…å®¹è¿‡çŸ­({len(content)}å­—ç¬¦)ï¼Œå°è¯•æŠ“å–å…¨æ–‡: {url}")
                full_text = fetch_full_text(url)
                if full_text:
                    content = full_text

            title = getattr(entry, "title", "æ— æ ‡é¢˜")
            author = getattr(entry, "author", source.name)

            article = Article(
                url=url,
                title=title,
                author=author,
                source_name=source.name,
                published_at=pub_time,
                content=content,
            )
            articles.append(article)
            logger.info(f"  å·²è·å–: {title} ({len(content)} å­—ç¬¦)")

        # ç¤¼è²Œç­‰å¾…ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(1)

    logger.info(f"å…±è·å– {len(articles)} ç¯‡æ–°æ–‡ç« ")
    return articles
